Results:
Results summary
Results in model_outputs/LambdaMart/5/hyperparameter_tuning/transformer_NDCG_kullback_leibler_divergence_project
Showing 10 best trials
Objective(name='val_mean_squared_error', direction='min')
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
final_activation: sigmoid
learning_rate: 1e-05
tuner/epochs: 20
tuner/initial_epoch: 7
tuner/bracket: 2
tuner/round: 2
tuner/trial_id: a76401d4319e0405bed8e353e785987b
Score: 0.19545522332191467
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
final_activation: sigmoid
learning_rate: 1e-05
tuner/epochs: 7
tuner/initial_epoch: 3
tuner/bracket: 2
tuner/round: 1
tuner/trial_id: 2494843abcdb2c67d425f99ea971b327
Score: 0.21220551431179047
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
final_activation: sigmoid
learning_rate: 1e-05
tuner/epochs: 3
tuner/initial_epoch: 0
tuner/bracket: 2
tuner/round: 0
Score: 0.2126145213842392
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
final_activation: sigmoid
learning_rate: 0.0001
tuner/epochs: 3
tuner/initial_epoch: 0
tuner/bracket: 2
tuner/round: 0
Score: 0.288049578666687
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: sigmoid
final_activation: sigmoid
learning_rate: 0.0001
tuner/epochs: 20
tuner/initial_epoch: 7
tuner/bracket: 1
tuner/round: 1
tuner/trial_id: abcf6d0f2902bf5fed943e1d466dcf4e
Score: 0.29078057408332825
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
final_activation: sigmoid
learning_rate: 0.0001
tuner/epochs: 20
tuner/initial_epoch: 7
tuner/bracket: 2
tuner/round: 2
tuner/trial_id: 097aeb65da54e31d5236203ae99b0086
Score: 0.2910286486148834
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
final_activation: sigmoid
learning_rate: 0.0001
tuner/epochs: 7
tuner/initial_epoch: 3
tuner/bracket: 2
tuner/round: 1
tuner/trial_id: 5b3481f8ebc59b2285f4bfc2f200e00b
Score: 0.2923335134983063
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: sigmoid
final_activation: sigmoid
learning_rate: 0.0001
tuner/epochs: 7
tuner/initial_epoch: 0
tuner/bracket: 1
tuner/round: 0
Score: 0.29274916648864746
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: relu
final_activation: sigmoid
learning_rate: 0.001
tuner/epochs: 3
tuner/initial_epoch: 0
tuner/bracket: 2
tuner/round: 0
Score: 0.3032412827014923
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: relu
final_activation: sigmoid
learning_rate: 0.001
tuner/epochs: 7
tuner/initial_epoch: 3
tuner/bracket: 2
tuner/round: 1
tuner/trial_id: aba974e3332da8cb607dca85edc5c58f
Score: 0.30339378118515015


Best Model Summary:
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
main_input (InputLayer)         [(None, 10, 720)]    0                                            
__________________________________________________________________________________________________
transformer-MultiHeadSelfAttent (None, 10, 720)      33212880    main_input[0][0]                 
                                                                 main_input[0][0]                 
__________________________________________________________________________________________________
transformer-MultiHeadSelfAttent (None, 10, 720)      0           main_input[0][0]                 
                                                                 transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
transformer-MultiHeadSelfAttent (None, 10, 720)      1440        transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
transformer-FeedForward (Dense) (None, 10, 720)      519120      main_input[0][0]                 
__________________________________________________________________________________________________
transformer-FeedForward-Add (Ad (None, 10, 720)      0           transformer-MultiHeadSelfAttentio
                                                                 transformer-FeedForward[0][0]    
__________________________________________________________________________________________________
transformer-FeedForward-Norm (L (None, 10, 720)      1440        transformer-FeedForward-Add[0][0]
__________________________________________________________________________________________________
flatten (Flatten)               (None, 7200)         0           transformer-FeedForward-Norm[0][0
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            7201        flatten[0][0]                    
==================================================================================================
Total params: 33,742,081
Trainable params: 33,742,081
Non-trainable params: 0
__________________________________________________________________________________________________



