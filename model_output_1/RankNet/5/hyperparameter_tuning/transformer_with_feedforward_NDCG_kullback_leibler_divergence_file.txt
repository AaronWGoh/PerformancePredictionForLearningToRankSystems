Results:
Results summary
Results in operational/RankNet/5/hyperparameter_tuning/transformer_with_feedforward_NDCG_kullback_leibler_divergence_project
Showing 10 best trials
Objective(name='val_mean_squared_error', direction='min')
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: tanh
first_neuron: 160
first_activation: softmax
second_neuron: 672
second_activation: relu
third_neuron: 352
third_activation: relu
fourth_activation: softmax
output_activation: sigmoid
learning_rate: 0.001
tuner/epochs: 3
tuner/initial_epoch: 0
tuner/bracket: 2
tuner/round: 0
Score: 0.06048818305134773
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: relu
first_neuron: 352
first_activation: relu
second_neuron: 672
second_activation: relu
third_neuron: 288
third_activation: softmax
fourth_activation: softmax
output_activation: sigmoid
learning_rate: 0.001
tuner/epochs: 20
tuner/initial_epoch: 7
tuner/bracket: 1
tuner/round: 1
tuner/trial_id: bb36b648b2db06fbde9c5989700c5dfd
Score: 0.06752252578735352
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
first_neuron: 160
first_activation: softmax
second_neuron: 736
second_activation: softmax
third_neuron: 736
third_activation: sigmoid
fourth_activation: softmax
output_activation: sigmoid
learning_rate: 1e-05
tuner/epochs: 20
tuner/initial_epoch: 7
tuner/bracket: 1
tuner/round: 1
tuner/trial_id: 7b5f3313e640728d40b9d21cc1dadfd1
Score: 0.07527132332324982
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: tanh
first_neuron: 224
first_activation: softmax
second_neuron: 224
second_activation: softmax
third_neuron: 416
third_activation: relu
fourth_activation: relu
output_activation: sigmoid
learning_rate: 0.01
tuner/epochs: 7
tuner/initial_epoch: 3
tuner/bracket: 2
tuner/round: 1
tuner/trial_id: 97d15da82c4bd2a5b1d7f84ef0133276
Score: 0.10077770054340363
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: tanh
first_neuron: 224
first_activation: softmax
second_neuron: 224
second_activation: softmax
third_neuron: 416
third_activation: relu
fourth_activation: relu
output_activation: sigmoid
learning_rate: 0.01
tuner/epochs: 20
tuner/initial_epoch: 7
tuner/bracket: 2
tuner/round: 2
tuner/trial_id: 671a8b35f3da758c6469796df944a675
Score: 0.10078848153352737
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: softmax
first_neuron: 160
first_activation: softmax
second_neuron: 736
second_activation: softmax
third_neuron: 736
third_activation: sigmoid
fourth_activation: softmax
output_activation: sigmoid
learning_rate: 1e-05
tuner/epochs: 7
tuner/initial_epoch: 0
tuner/bracket: 1
tuner/round: 0
Score: 0.10693304985761642
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: relu
first_neuron: 352
first_activation: relu
second_neuron: 672
second_activation: relu
third_neuron: 288
third_activation: softmax
fourth_activation: softmax
output_activation: sigmoid
learning_rate: 0.001
tuner/epochs: 7
tuner/initial_epoch: 0
tuner/bracket: 1
tuner/round: 0
Score: 0.11673112213611603
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: tanh
first_neuron: 160
first_activation: softmax
second_neuron: 672
second_activation: relu
third_neuron: 352
third_activation: relu
fourth_activation: softmax
output_activation: sigmoid
learning_rate: 0.001
tuner/epochs: 20
tuner/initial_epoch: 7
tuner/bracket: 2
tuner/round: 2
tuner/trial_id: 33226875026ddf585b872fe7509a1602
Score: 0.17509135603904724
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 8
feedforward_act: tanh
first_neuron: 160
first_activation: softmax
second_neuron: 672
second_activation: relu
third_neuron: 352
third_activation: relu
fourth_activation: softmax
output_activation: sigmoid
learning_rate: 0.001
tuner/epochs: 7
tuner/initial_epoch: 3
tuner/bracket: 2
tuner/round: 1
tuner/trial_id: aa5ba028e1a1177e2a79bde6ead73200
Score: 0.19239768385887146
Trial summary
Hyperparameters:
num_batches: 10
num_features: 720
loss: kullback_leibler_divergence
num_heads: 16
feedforward_act: tanh
first_neuron: 224
first_activation: softmax
second_neuron: 224
second_activation: softmax
third_neuron: 416
third_activation: relu
fourth_activation: relu
output_activation: sigmoid
learning_rate: 0.01
tuner/epochs: 3
tuner/initial_epoch: 0
tuner/bracket: 2
tuner/round: 0
Score: 0.21490606665611267


Best Model Summary:
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
main_input (InputLayer)         [(None, 10, 720)]    0                                            
__________________________________________________________________________________________________
transformer-MultiHeadSelfAttent (None, 10, 720)      16606800    main_input[0][0]                 
                                                                 main_input[0][0]                 
__________________________________________________________________________________________________
transformer-MultiHeadSelfAttent (None, 10, 720)      0           main_input[0][0]                 
                                                                 transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
transformer-MultiHeadSelfAttent (None, 10, 720)      1440        transformer-MultiHeadSelfAttentio
__________________________________________________________________________________________________
transformer-FeedForward (Dense) (None, 10, 720)      519120      main_input[0][0]                 
__________________________________________________________________________________________________
transformer-FeedForward-Add (Ad (None, 10, 720)      0           transformer-MultiHeadSelfAttentio
                                                                 transformer-FeedForward[0][0]    
__________________________________________________________________________________________________
transformer-FeedForward-Norm (L (None, 10, 720)      1440        transformer-FeedForward-Add[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10, 160)      115360      transformer-FeedForward-Norm[0][0
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 10, 672)      108192      dense_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 10, 352)      236896      dense_2[0][0]                    
__________________________________________________________________________________________________
fourth_neuron (Dense)           (None, 10, 1)        353         dense_3[0][0]                    
__________________________________________________________________________________________________
flatten (Flatten)               (None, 10)           0           fourth_neuron[0][0]              
__________________________________________________________________________________________________
predictions (Dense)             (None, 1)            11          flatten[0][0]                    
==================================================================================================
Total params: 17,589,612
Trainable params: 17,589,612
Non-trainable params: 0
__________________________________________________________________________________________________



